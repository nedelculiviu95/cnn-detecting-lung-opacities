{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prototype.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedelculiviu95/cnn-detecting-lung-opacities/blob/master/prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqC6KeaPg2a4",
        "colab_type": "code",
        "outputId": "02fe0b58-26b7-472a-c158-971916d4c600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install pydicom\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import glob\n",
        "import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from random import randint\n",
        "from pathlib import Path\n",
        "import os\n",
        "import pydicom\n",
        "from skimage import measure\n",
        "from skimage.transform import resize\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "import csv\n",
        "from keras.callbacks import *\n",
        "from os.path import isfile, join\n",
        "from PIL import Image, ImageFile\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (1.2.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldU_ugRkB4pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "BATCHSIZE = 16\n",
        "CHANNELS = 64\n",
        "IMAGE_SIZE = 224\n",
        "NBLOCK = 6 \n",
        "DEPTH = 6\n",
        "MOMENTUM = 0.9\n",
        "DIMS = (IMAGE_SIZE,IMAGE_SIZE,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CnV_PKu5NwS",
        "colab_type": "code",
        "outputId": "d5bce5d4-eabb-425d-91ac-a0cd61656d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "images1_folder = \"/content/gdrive/My Drive/images1/\"\n",
        "images3_folder = \"/content/gdrive/My Drive/images/\"\n",
        "filenames_3 = [f for f in os.listdir(images3_folder) if isfile(join(images3_folder, f))]\n",
        "filenames_1 = [f for f in os.listdir(images1_folder) if isfile(join(images1_folder, f))]\n",
        "\n",
        "train_filenames = []\n",
        "folder = \"/content/gdrive/My Drive/data_filenames/train.txt\"\n",
        "file1 = open(folder, \"r\") \n",
        "for line in file1: \n",
        "  l = line[0:16]\n",
        "  train_filenames.append(l)\n",
        "file1.close()\n",
        "\n",
        "print('n train samples', len(train_filenames))\n",
        "\n",
        "test_filenames = []\n",
        "folder = \"/content/gdrive/My Drive/data_filenames/test.txt\"\n",
        "file1 = open(folder, \"r\") \n",
        "for line in file1: \n",
        "  l = line[0:16]\n",
        "  test_filenames.append(l)\n",
        "file1.close()\n",
        "\n",
        "print('n test samples', len(test_filenames))\n",
        "\n",
        "valid_filenames = []\n",
        "folder = \"/content/gdrive/My Drive/data_filenames/validation.txt\"\n",
        "file1 = open(folder, \"r\") \n",
        "for line in file1: \n",
        "  l = line[0:16]\n",
        "  valid_filenames.append(l)\n",
        "file1.close()\n",
        "\n",
        "print('n validation samples', len(valid_filenames))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n train samples 11665\n",
            "n test samples 2729\n",
            "n validation samples 1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G46Ipu72xdJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_data(df):\n",
        "    parsed = {}\n",
        "    for n, row in df.iterrows():\n",
        "        # --- Initialize patient entry into parsed \n",
        "        pid = row['Image Index']\n",
        "        if pid not in parsed:\n",
        "            labels = row['Finding Labels']\n",
        "            parsed[pid] = {\n",
        "                'png': '%s' % pid,\n",
        "                'label': labels.split(\"|\"),\n",
        "                'patient_gender': row['Patient Gender'],\n",
        "                'pattient_age': row['Patient Age']}\n",
        "        \n",
        "        #labels = row['Finding Labels']\n",
        "        #parsed[pid]['label'].append(labels.split(\"|\"))\n",
        "        \n",
        "    return parsed\n",
        "  \n",
        "  \n",
        "df = pd.read_csv('/content/gdrive/My Drive/ChestXRay_csv/Data_Entry_2017.csv')\n",
        "parsed = parse_data(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqvI5mcKxiZF",
        "colab_type": "code",
        "outputId": "d7ec2c7f-bff2-4601-edd4-bb60aeebd60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def concrete_labels(parsed, filenames):\n",
        "  n_labels = 14\n",
        "  labels_array = np.zeros(len(filenames), dtype=int) #4999 x 14\n",
        "  dictionary = {'Atelectasis': 0, 'Cardiomegaly': 1, 'Effusion': 2, 'Infiltration':3, \n",
        "                'Mass': 4, 'Nodule': 5, 'Pneumonia': 6, 'Pneumothorax': 7, 'Consolidation': 8,\n",
        "               'Edema':9, 'Emphysema':10, 'Fibrosis': 11, 'Pleural_Thickening': 12, 'Hernia': 13}\n",
        "  i = 0\n",
        "  target = 1\n",
        "  for item in filenames:\n",
        "    diseases = parsed[item]['label']\n",
        "    for d in diseases:\n",
        "      if d == 'No Finding':\n",
        "        target = 0\n",
        "        break\n",
        "      else:\n",
        "        target = 1\n",
        "        break\n",
        "    labels_array.append(target)\n",
        "   \n",
        "  return labels_array\n",
        "\n",
        "def get_label(filename, parsed):\n",
        "  n_labels = 14\n",
        "  label_array = np.zeros((1, n_labels), dtype=int) #1 x 14\n",
        "  diseases = parsed[filename]['label']\n",
        "  target = 1\n",
        "  for d in diseases:\n",
        "      if d == 'No Finding':\n",
        "        target = 0\n",
        "        break\n",
        "      else:\n",
        "        target = 1\n",
        "        break\n",
        "  return target\n",
        "\n",
        "yy = get_label(train_filenames[2], parsed)\n",
        "print(\"filename: \", train_filenames[2] )\n",
        "print(\"label: \", yy )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filename:  00003923_016.png\n",
            "label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nUGQOUixrld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##data generator\n",
        "labels = ['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', \n",
        "                'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation',\n",
        "               'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
        "class data_generator(keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, folder, filenames, parsed, batch_size=BATCHSIZE, image_size=IMAGE_SIZE, shuffle=True, augment=False, predict=False):\n",
        "        self.folder = folder\n",
        "        self.filenames = filenames\n",
        "        self.parsed = parsed\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.predict = predict\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __load__(self, filename):\n",
        "        img_path = os.path.join(self.folder, filename)\n",
        "        img = Image.open(img_path)\n",
        "        img_arr = np.asarray(img)\n",
        "        label = 1\n",
        "        diseases = parsed[filename]['label']\n",
        "        if diseases[0] == 'No Finding':\n",
        "            label = 0\n",
        "        img_arr = resize(img_arr, (self.image_size, self.image_size), mode='reflect')\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            img = np.fliplr(img)=\n",
        "        img = np.expand_dims(img_arr, -1)\n",
        "        return img, label\n",
        "      \n",
        "    def __loadpredict__(self, filename):\n",
        "        img_path = os.path.join(self.folder, filename)\n",
        "        img = Image.open(img_path)\n",
        "        img_arr = np.asarray(img)\n",
        "        img = resize(img_arr, (self.image_size, self.image_size), mode='reflect')\n",
        "        img = np.expand_dims(img, -1)\n",
        "        return img, label\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        if self.predict:\n",
        "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
        "            imgs = np.array(imgs)\n",
        "            return imgs, filenames\n",
        "        else:=\n",
        "            items = [self.__load__(filename) for filename in filenames]\n",
        "            imgs, labels = zip(*items)\n",
        "            imgs = np.array(imgs)\n",
        "            labels = np.array(labels)\n",
        "            return imgs, labels\n",
        "    def on_epoch_end(self,):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.filenames)\n",
        "    def __len__(self):\n",
        "        if self.predict:\n",
        "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
        "        else:\n",
        "            return int(len(self.filenames) / self.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvGW8iuZjnV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## create model\n",
        "from keras.layers import BatchNormalization, ReLU, Conv2D, AveragePooling2D, MaxPool2D, GlobalAveragePooling2D, Dense\n",
        "\n",
        "def pooling_block(inputs, pool=2):\n",
        "    x = BatchNormalization(momentum=MOMENTUM)(inputs)\n",
        "    x = ReLU(0)(x)\n",
        "    x = MaxPool2D(pool)(x)\n",
        "    return x\n",
        "  \n",
        "def composite_function(channels, inputs, size=3, padding='same'):\n",
        "    x = BatchNormalization(momentum=MOMENTUM)(inputs)\n",
        "    x = ReLU(0)(x)\n",
        "    x = Conv2D(channels, size, padding=padding, use_bias=False)(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(inputs, channels1, channels2):\n",
        "    x = composite_function(channels1, inputs)\n",
        "    x = composite_function(channels2, x)\n",
        "    x = keras.layers.Concatenate()([inputs, x])\n",
        "    return x\n",
        "  \n",
        "def dense_block(inputs, nblocks=6, channels1=128, channels2=32):\n",
        "    x = inputs\n",
        "    for i in range(nblocks):\n",
        "        x = conv_block(x, channels1, channels2)\n",
        "    return x\n",
        "\n",
        "def transition_block(inputs, channels, pool=2):\n",
        "    x = composite_function(channels, inputs)\n",
        "    x = AveragePooling2D(pool)(x)\n",
        "    return x\n",
        "\n",
        "def build_model(input_size, channels=64, channels2=32, n_blocks=NBLOCK, depth=DEPTH):\n",
        "    input1 = keras.Input(shape=(input_size, input_size, 1))\n",
        "    x = Conv2D(channels, 3, padding='same', strides=2, use_bias=False)(input1)\n",
        "    x = pooling_block(x)\n",
        "    nchan = channels\n",
        "    for d in range(depth-3):\n",
        "        x = dense_block(x)\n",
        "        nchan = ( nchan + n_blocks*channels2 ) // 2\n",
        "        x = transition_block(x, nchan)\n",
        "    x = dense_block(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = (0)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    model = keras.Model(inputs=input1, outputs=output)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pva2rksYj6Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create network and compiler\n",
        "model = build_model(input_size=IMAGE_SIZE, channels=CHANNELS, n_blocks=NBLOCK, depth=DEPTH)\n",
        "#103 layers\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=LR),\n",
        "              loss=keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
        "train_gen = generator(images3_folder, train_filenames, parsed, batch_size=BATCHSIZE, \n",
        "                      image_size=IMAGE_SIZE, shuffle=True, augment=True, predict=False)\n",
        "valid_gen = generator(images1_folder, valid_filenames, parsed, batch_size=BATCHSIZE,\n",
        "                      image_size=IMAGE_SIZE, shuffle=False, predict=False)\n",
        "\n",
        "history = model.fit_generator(train_gen, validation_data=valid_gen, \n",
        "                              epochs=EPOCHS, shuffle=True, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYxBfcnyazjT",
        "colab_type": "text"
      },
      "source": [
        "# Testing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLhE5c4w8l2F",
        "colab_type": "text"
      },
      "source": [
        "##1)Testing one image "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziMbzPa4BEGx",
        "colab_type": "code",
        "outputId": "a7b60e7f-71ff-4499-f566-cbdc6681ebac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#testing if img has pneumonia or not \n",
        "feature = model.predict(img)\n",
        "print(feature)\n",
        "if feature[0][0] > 0.5:\n",
        "  print(\"Prediction: patient\",filename, \"has pneumonia\")\n",
        "else:\n",
        "  print(\"Prediction: patient\",filename, \"doesn't have pneumonia\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.21551116]]\n",
            "Prediction: patient 7dfe6d14-a31c-483a-be83-7a705e45719f doesn't have pneumonia\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72XXKl1jbGBL",
        "colab_type": "text"
      },
      "source": [
        "#Plotting training and validation matrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-SERvUxy7fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}